---
tags: post,industry
layout: post
title: Inefficiency Drives Growth
---

I want to talk about growth in the tech industry...

Since the advent of computing, hardware manufacturers competed by making their machines smaller, faster, and more efficient. [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law) was soon observed, showing that transistor counts doubled roughly every 2 years.

Throughout most of this, software would keep pace and increase in complexity and capabilities, using more of the available hardware, eventually requiring more of it, pushing consumers to continually upgrade to get the newest features.

But something weird happened. Consumer's _need_ for more power slowed significantly. Additionally, Moore's Law also seems to have died. What's a tech company to do when that happens? The answer it seems, is to manufacture demand.

You can see this most obviously with games. Big game companies relentlessly chase realism and fidelity, cramming higher resolution textures and effects and pixels far beyond what any normal person is even capable of noticing, driving a need for the latest and greatest computers just to play their titles. Nvidia even did the absurd thing and brought real-time raytracing to their chips, an incredibly inefficient way to render 3d scenes to a screen, in an effort to continually drive the need for more and more chips. And because of the death of Moore's Law, the only way to justify these forced upgrades is to throw more electrons at it to beef up their performance numbers. Current Nvidia cards are massive behemoths with wattages that rival cooking appliances. The fact that the most common Nvidia card currently used by consumers is 2 generations old should tell you how poorly this strategy has been working for them.

But this happens with other tech as well. Smartphones don't do much more than they did 10 years ago, but you still need to buy a new one every 2 years, and it's not just because of the manufacturer killing it through lack of updates. Mobile networks drop support for older but perfectly capable standards, apps become more inefficient, requiring better hardware, the web is a garbage fire of bloated, inefficient frontend frameworks that consume RAM like its going out of style. Every "native" app these days ships with an entire browser and runs interpreted javascript.

The big one these days is LLMs. They're being positioned as the messiah, a "new paradigm of computing" that allows anyone to run "software written entirely in English". They're even calling this "software 3.0" (1.0 being software, 2.0 being more traditional ML). Though it has some legitimate uses, it's mostly a way to do computing tasks _incredibly inefficiently_ and usually unreliably. Since it requires ridiculous amounts of hardware to run, however, it means the hardware manufacturers (Nvidia) can continue their endless growth of hardware nobody actually needs. Not only that, the companies making and operating these models can now seek rent on anyone using them, since they're the only ones with the infrastructure to operate them.

Some like to say that private corporations are more efficient, but I say they grow by breeding inefficiencies. More middle-men, worse services, dark patterns, bloated and slow software requiring more power hungry hardware. This is especially true in our new techno-feudal future we now find ourselves in, where competition is dead and the few at the top hold all the cards.

Merry Christmas I guess.
